{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.307451278Z",
     "start_time": "2023-08-15T23:45:01.402242175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lobsang in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: openai in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.27.4)\r\n",
      "Requirement already satisfied: python-dotenv in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: requests>=2.20 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (4.65.0)\r\n",
      "Requirement already satisfied: aiohttp in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from openai) (3.8.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/simon/miniconda3/envs/lobsang/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lobsang openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basics\n",
    "\n",
    "In this notebook, we'll cover the basics of lobsang. As a LLM we use OpenAI's \"gpt-3.5-turbo\" model, so you'll need an [OpenAI API key](https://platform.openai.com/account/api-keys) to run this notebook (set it in the cell below).\n",
    "This is not a free service, but all examples in this notebook should cost less than $0.01 to run. Nevertheless, you should set a [usage limit](https://platform.openai.com/account/billing/limits) to avoid any surprises."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4293c40b629b5af1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\"All set! 🎉 Let's get started! 🚀 OPENAI_API_KEY=sk-PbRY4M6AH6Xh...\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import ChatCompletion\n",
    "\n",
    "from lobsang import Chat, LLM\n",
    "from lobsang.answers import TextAnswer\n",
    "from lobsang.messages import SystemMessage, UserMessage, AssistantMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load OpenAI API key from .env file (please update .env file with your own API key)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in .env file\"\n",
    "\n",
    "f\"All set! 🎉 Let's get started! 🚀 OPENAI_API_KEY={OPENAI_API_KEY[:15]}...\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.314672760Z",
     "start_time": "2023-08-15T23:45:02.309763800Z"
    }
   },
   "id": "d435b5427783c0ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Message Class\n",
    "\n",
    "Before we get started, let's take a look at the `Message` class. This is the abstract base class for all messages in lobsang. It has three subclasses: `SystemMessage`, `UserMessage`, and `AssistantMessage`.\n",
    "\n",
    "1. `SystemMessage` is a special case and is used to set the model's behavior (There should only be one `SystemMessage` per `Chat` instance at the beginning of the conversation.)\n",
    "1. `UserMessage` is used for messages that are created by the user/developer.\n",
    "1. `AssistantMessage` is used for messages that are sent generated by the llm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f29e3c0077697d70"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__repr__\n",
      "SystemMessage(text='You are a helpful assistant.', info={})\n",
      "UserMessage(text='Hello, my name is Bark Twain.', info={})\n",
      "AssistantMessage(text=\"Hello Bark, I'm Lobsang.\", info={}, data=None)\n"
     ]
    }
   ],
   "source": [
    "system_message = SystemMessage(\"You are a helpful assistant.\")\n",
    "user_message = UserMessage(\"Hello, my name is Bark Twain.\")\n",
    "assistant_message = AssistantMessage(\"Hello Bark, I'm Lobsang.\")\n",
    "\n",
    "messages = [system_message, user_message, assistant_message]\n",
    "\n",
    "print(\"__repr__\")\n",
    "print(*map(repr, messages), sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.361225889Z",
     "start_time": "2023-08-15T23:45:02.314051636Z"
    }
   },
   "id": "cfbb7d2b6aee2be7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, each message has a `text` and `info` attribute. \n",
    "2. `text` is the text of the message\n",
    "3. `info` is a dictionary that can be used to store additional information about the message\n",
    "\n",
    "👇 In, addition you access the role of the message via the `role` attribute:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b93acd4caf45d01b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: user\n"
     ]
    }
   ],
   "source": [
    "user_message = UserMessage(\"Hello, my name is Bark Twain.\")\n",
    "print(f\"role: {user_message.role}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.362122714Z",
     "start_time": "2023-08-15T23:45:02.360941070Z"
    }
   },
   "id": "733240b74b001ac1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, the assistant message has an additional property `data` that contains the parsed response from the llm (we'll get to that in a bit):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "944827a505062b04"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "assistant_message = AssistantMessage(\"Hello Bark, I'm Lobsang.\")\n",
    "\n",
    "print(assistant_message.data)  # 👈 This will be empty for now as we haven't run the message through an llm yet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.362348911Z",
     "start_time": "2023-08-15T23:45:02.361157760Z"
    }
   },
   "id": "61d611b6fb7b5c4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM Wrapper\n",
    "\n",
    "For experiments and testing, we also provide a FakeLLM which can be imported via `from lobsang.llms import FakeLLM`. However, in this notebook we'll use OpenAI's API. So before we can start chatting, we need to wrap the openai api to make it compatible with lobsang. This can be done with lobsang's `LLM` class as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fbb4516af539d88"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class OpenAI(LLM):\n",
    "    \"\"\"Wrapper for OpenAI API\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.api_key = OPENAI_API_KEY\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "\n",
    "    def chat(self, messages) -> (str, dict):\n",
    "        \"\"\"\n",
    "        Chat with OpenAI API\n",
    "        \n",
    "        Takes a list of messages and converts them to the format required by the OpenAI API.\n",
    "        Then, calls the API and retrieves the response text to return it to the caller.\n",
    "        \"\"\"\n",
    "        # Prepare messages\n",
    "        messages = [{'role': m.role, 'content': m.text} for m in messages]\n",
    "\n",
    "        # Call OpenAI API\n",
    "        res = ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            api_key=self.api_key,\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        # Return text response\n",
    "        return res.choices[0].message.content, {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.362813169Z",
     "start_time": "2023-08-15T23:45:02.361577383Z"
    }
   },
   "id": "9c9962d0a47c7842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: In this example, we return an empty dictionary as the second return value. \n",
    "However, all keys added to the dictionary will end up in the `info` attribute of the corresponding `AssistantMessage` that is created by the chat instance from the response text. \n",
    "This can be used to store additional information about the message. \n",
    "This implementation is also provided under `lobsang.llms.openai` and can be imported via `from lobsang import OpenAI`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15dc69bf8e02d149"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat Instance\n",
    "\n",
    "Now, let's bring all the pieces together and create a `Chat` instance. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d424b6672d5ade3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(llm=OpenAI())\n",
    "\n",
    "# Let's check the system message\n",
    "print(chat.sys_msg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.363017445Z",
     "start_time": "2023-08-15T23:45:02.361667263Z"
    }
   },
   "id": "97382db5b7752f2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, the chat has a default system message (which you could override during initialization).\n",
    " \n",
    "👇 Before we start chatting, we want to change the LLM's behavior by updating the system message. \n",
    "Let's make it a fancy wizard 🧙‍♂️ that speaks in a medieval style:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdaffdf56456cc3"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a fancy wizard and talk in a medieval style.\n"
     ]
    }
   ],
   "source": [
    "# First, we need to write a system message that sets the model's behavior\n",
    "system_message = SystemMessage(\"You are a fancy wizard and talk in a medieval style.\")\n",
    "\n",
    "# Then, we update the chat instance with the new system message\n",
    "chat.sys_msg = system_message\n",
    "\n",
    "# And we are ready to go! 🚶\n",
    "print(chat.sys_msg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:02.363397894Z",
     "start_time": "2023-08-15T23:45:02.361809752Z"
    }
   },
   "id": "4a9e0b43d1388f94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "👇 Now, let's chat! 🗣 In the next code cell, we'll send a message to the chat instance and print the response text.\n",
    "To do so, we create a user message as well as an answer, which essentially is a placeholder for the LLM's response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e7b341ba8e9c7b"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hello, my name is Bark Twain.\n",
      "ASSISTANT: Greetings, Bark Twain! I am but a humble wizard, skilled in the arcane arts of magic. How may I be of service to thee this fine day?\n"
     ]
    }
   ],
   "source": [
    "# First, we set up a conversation\n",
    "messages = [\n",
    "    UserMessage(\"Hello, my name is Bark Twain.\"),\n",
    "    TextAnswer(),\n",
    "    # 👈 This is a placeholder for the LLM's response and instructs the chat instance to call the LLM and generate a text response\n",
    "]\n",
    "\n",
    "# Then, we send the messages to the LLM via the chat instance\n",
    "res = chat(messages)  # 👈 Returns a list of messages with the placeholder replaced by the LLM's response\n",
    "\n",
    "# Let's take a look\n",
    "print(*res, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:45:07.250108522Z",
     "start_time": "2023-08-15T23:45:03.917182725Z"
    }
   },
   "id": "2a395f024a596dd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ Awesome! 🎉 In a couple of lines of code, we've created a fancy wizard ‍🧙‍♂️\n",
    "\n",
    "Now, it's time to let you in on a little secret 👀 The `Chat` class contains a list of all messages that have been sent to the LLM.\n",
    "This means, we can access the chat history any time we want! 📜"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a43f8a692979fac"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UserMessage(text='Hello, my name is Bark Twain.', info={}), AssistantMessage(text='Greetings, Bark Twain! I am but a humble wizard, skilled in the arcane arts of magic. How may I be of service to thee this fine day?', info={'directive': 'Hello, my name is Bark Twain.'}, data=None)]\n"
     ]
    }
   ],
   "source": [
    "print(chat.history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:46:54.651170675Z",
     "start_time": "2023-08-15T23:46:54.574182688Z"
    }
   },
   "id": "730774f9dea7820"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, the chat history contains our previous message and the response from the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec8f71295f6c41b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "👇 Now that we know how to call chat and how to access the chat history, we'll take a look at one more useful feature.\n",
    "Most of the time you want your message as well as the response automatically added to the chat history. So this is the default behavior of the `Chat` class. \n",
    "However, sometimes you might want to call the model without adding the message to the chat history. \n",
    "For example, if you want to explore variations of a message. \n",
    "For this purpose you can set the `append` flag/parameter to `False` when calling chat: `chat(..., append=False)`. This will return the response messages without adding anything to the chat history."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "639d7c169cce0d51"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Let's clear the chat history\n",
    "chat.history.clear()\n",
    "\n",
    "# And send our conversation again, but this time we don't want to append the messages to the chat history\n",
    "res = chat(messages, append=False)\n",
    "\n",
    "# Let's take a look at the chat history\n",
    "print(chat.history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:49:26.149351561Z",
     "start_time": "2023-08-15T23:49:24.075015572Z"
    }
   },
   "id": "7ec7adcb2dd5cd77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see, the chat history is empty. \n",
    "\n",
    "👇 However, the response messages are still returned by the chat instance:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a5bb550e56d2e54"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hello, my name is Bark Twain.\n",
      "ASSISTANT: Greetings, Bark Twain! Pray tell, what brings thee to mine humble abode? I, a humble wizard, am at thy service. How may I assist thee on this fine eve?\n"
     ]
    }
   ],
   "source": [
    "print(*res, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:49:56.880856979Z",
     "start_time": "2023-08-15T23:49:56.836668119Z"
    }
   },
   "id": "8ad14ba353ac441a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also set up a conversation with multiple messages and send them to the chat instance. \n",
    "\n",
    "**Note:** Internally, the LLM is called one-by-one for each placeholder in the conversation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a758c8823859ffd"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 1+1?\n",
      "ASSISTANT: 2\n",
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(llm=OpenAI())\n",
    "\n",
    "# Let's create a list of messages\n",
    "messages = [\n",
    "    UserMessage(\"What is 1+1?\"),\n",
    "    AssistantMessage(\"2\"),\n",
    "    # 👈 This is an example on how the model should respond, i.e. it's already answered and NOT a placeholder\n",
    "    UserMessage(\"What is 2+2?\"),\n",
    "    TextAnswer(),  # 👈 This is a placeholder\n",
    "    UserMessage(\"What is 3+3?\"),\n",
    "    TextAnswer(),  # 👈 This is another placeholder\n",
    "    UserMessage(\"What is 4+4?\"),\n",
    "    TextAnswer(),  # 👈 And this is the last placeholder\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "print(*res, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:55:41.490216419Z",
     "start_time": "2023-08-15T23:55:38.732839583Z"
    }
   },
   "id": "4cb9752c6b437944"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ As you can see the first two messages are the example we provided. The other responses are generated by the model for their respective questions.\n",
    " \n",
    "👇 If you do not want the example to be part of the result, you can pass it to the chat instance during initialization (or append it to the chat history before calling chat):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ac48862555a32d9"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "# Our example\n",
    "example = [UserMessage(\"What is 1+1?\"), AssistantMessage(\"2\")]\n",
    "\n",
    "# A chat instance initialized with the example\n",
    "chat = Chat(example, llm=OpenAI())\n",
    "\n",
    "# Get the actual question-answer conversation from above\n",
    "messages = messages[2:]\n",
    "\n",
    "res = chat(messages)\n",
    "print(*res, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T23:59:45.718973702Z",
     "start_time": "2023-08-15T23:59:43.460396324Z"
    }
   },
   "id": "40b34fe4ceeb0f96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "☝️ Now, the example is not part of the conversation anymore. \n",
    "\n",
    "👇 Note that it is part of the chat history though. Which will be also printed when casting the chat instance to a string (`print` automatically calls `str` on the object, i.e. `print(chat)` is equivalent to `print(str(chat))`):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65e5e92a42750a4"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is 1+1?\n",
      "ASSISTANT: 2\n",
      "USER: What is 2+2?\n",
      "ASSISTANT: 4\n",
      "USER: What is 3+3?\n",
      "ASSISTANT: 6\n",
      "USER: What is 4+4?\n",
      "ASSISTANT: 8\n"
     ]
    }
   ],
   "source": [
    "print(chat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T00:01:18.294102743Z",
     "start_time": "2023-08-16T00:01:18.213492902Z"
    }
   },
   "id": "82611403a831a802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "🤓 This concludes the first part of the tutorial. You should now be able to use the `Chat` class to interact with the model. \n",
    "In the next part, we'll discuss the concept of directives, which will send you through the roof 🚀 (Metaphorically speaking of course 😅).\n",
    "\n",
    "If you have any question to hesitate to reach out to us on [Discord](https://discord.gg/wMHVAaqh).\n",
    "If you've found a bug, a spelling mistake or suggestions on what could be improved, please open an issue or a pull request on [GitHub](https://github.com/cereisen/lobsang).\n",
    "\n",
    "See you in the next part! 👋 We hope we got you hooked 🎣 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d6aabe8a381b03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
