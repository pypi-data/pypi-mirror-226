import uuid
import json
import base64
import logging
import threading
from collections import OrderedDict, defaultdict
from datetime import datetime
from typing import Type
from functools import wraps
from xia_engine.base import BaseEngine, BaseDocument, MetaEngine


def result_limiter(func):
    @wraps(func)
    def wrapper(cls, *args, **kwargs):
        _limit = kwargs["_limit"]
        gen = func(cls, *args, **kwargs)
        for i, item in enumerate(gen):
            yield item
            if i + 1 >= _limit:
                break
    return wrapper


class Engine(BaseEngine):
    """

    """
    @classmethod
    def connect(cls, document_class: Type[BaseDocument] = None):
        """Connect to the engine

        Args:
            document_class: (`subclass` of `BaseDocument`): Document definition

        Returns:
            Connection
        """
        address = document_class.get_address(cls) if document_class else None
        default_db_name = "" if cls.engine_db_shared else document_class.__name__
        if cls.engine_connector_class:
            default_connector_param = cls.engine_connector_class()
        else:
            default_connector_param = cls.engine_default_connector_param
        if not address:
            cls._database[default_db_name] = cls.engine_connector(**default_connector_param)
            return cls._database[default_db_name]
        else:
            default_db_name = address.get("_db", default_db_name)
            connect_param = {k: v for k, v in address.items() if not k.startswith("_")}
            connect_param = cls.engine_default_connector_param if not connect_param else connect_param
            if cls.engine_connector_class:
                connector_object = cls.engine_connector_class.from_display(**connect_param)
                connect_param = connector_object.get_display_data(
                    lazy=False, show_hidden=True
                )
            cls._database[default_db_name] = cls.engine_connector(**connect_param)
            return cls._database[default_db_name]

    @classmethod
    def get_connection(cls, document_class: Type[BaseDocument] = None):
        """Get engine connectionã€‚ Always using existed one when it is possible

        Args:
            document_class: (`subclass` of `BaseDocument`): Document definition

        Returns:
            Connection
        """
        address = document_class.get_address(cls) if document_class else None
        default_db_name = "" if cls.engine_db_shared else document_class.__name__
        if not address:
            if default_db_name not in cls._database:
                return cls.connect(document_class)
            else:
                return cls._database[default_db_name]
        db_name = address.get("_db", default_db_name)
        if db_name not in cls._database:
            return cls.connect(document_class)
        return cls._database[db_name]

    @classmethod
    def lock(cls, document_class: Type[BaseDocument], doc_id: str, timeout: int = None):
        """Lock entries for write

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            doc_id (str): Having predefined doc id, None means could be generated by engine
            timeout: Timeout for lock

        Returns:
            return True amd empty message if lock is successful, else false with error message

        Comments:
            Lock need based engine implementation
        """
        raise NotImplemented

    @classmethod
    def unlock(cls, document_class: Type[BaseDocument], doc_id: str):
        """Release the for write

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            doc_id (str): Having predefined doc id, None means could be generated by engine

        Returns:
            return True amd empty message if lock is successful, else false with error message

        Comments:
            Unlock need based engine implementation
        """
        raise NotImplemented

    @classmethod
    def batch(cls, operations: list, originals: dict):
        """Data Batch Modification

        The data will be updated at once or rolled back

        Args:
            operations: List of operations to be done
                * op: Operation type. "S" = set, "I" = create, "D" = delete, "U" = update
                * cls: Document Class
                * doc_id: Document ID
                * content: Document Content in Database form
            originals:  Dictionary (Help to roll back)
                * class: document class name
                * id: document id
                * content: document db form

        Returns:
            return True amd empty message if batch is successful, else False with error message
        """
        raise NotImplemented

    @classmethod
    def db_to_display(cls, document_class: Type[BaseDocument], db_content: dict
                      , lazy: bool = True, catalog: dict = None, show_hidden: bool = False):
        """Convert data from database form to display form

        Args:
            document_class: Document class
            db_content: Database Content
            lazy: Lazy Mode
            catalog: Data Catalog
            show_hidden: Show hidden member or not

        Returns:
            document in display form
        """
        doc = document_class.from_db(cls, **db_content)
        return doc.get_display_data(lazy=lazy, catalog=catalog, show_hidden=show_hidden)

    @classmethod
    def create_collection(cls, document_class: Type[BaseDocument]):
        """Create Collection if needed

        Args:
            document_class: document_class
        """

    @classmethod
    def replicate(cls, document_class: Type[BaseDocument], task_list: list):
        """Data replication

        Args:
            document_class: Python class of document
            task_list:  List of dictionary with the following keys:
                * id: document id
                * content: document db form
                * op: operation type: "I" for insert, "D" for delete, "U" for update, "L" for load

        Returns:
            task_results: List of dictionary with the following keys:
                * id: document id
                * op: operation type: "I" for insert, "D" for delete, "U" for update, "L" for load
                * time: time when data is replicated
                * status: status code of HTTP protocol
        """
        task_result = []
        for task in task_list:
            try:
                if task["op"] in ["I", "U", "L"]:
                    cls.set(document_class, task["id"], task["content"])
                elif task["op"] == "D":
                    cls.delete(document_class, task["id"])
                task_result.append({"id": task["id"], "op": task["op"], "msg": "",
                                    "time": datetime.now().timestamp(), "status": 200})
            except Exception as e:
                logging.exception(f"Replication Exception: {document_class.__name__}: {task['id']}-{task['op']}",
                                  exc_info=True)
                task_result.append({"id": task["id"], "op": task["op"], "msg": str(e),
                                    "time": datetime.now().timestamp(), "status": 500})
        return task_result

    @classmethod
    def backup(cls, document_class: Type[BaseDocument], location: str = None, data_encode: str = None,
               data_format: str = None, data_store: str = None, **kwargs):
        """Backup data of a model. The real implementation must use kwargs to distribute loads

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition4
            data_encode (str): Backup Data Code
            data_format (str): Backup Data Format
            data_store (str): Backup Data Store location
            location(str): Data location to e used by data store
            **kwargs: parameter to be passed at engine level
        """
        assert cls.backup_coder,  "Backup encoder must be set to use backup function"
        assert cls.backup_storer,  "Backup store must be set to use backup function"
        data_encode = data_encode if data_encode else cls.backup_coder.default_encode
        data_format = data_format if data_format else cls.backup_coder.default_format
        data_store = data_store if data_store else cls.backup_storer.default_store
        encoder = cls.backup_coder(document_class, data_encode=data_encode, data_format=data_format)
        storer = cls.backup_storer(location=location, data_store=data_store)
        doc_list = document_class.objects(_limit=2**20)
        for file_obj in storer.get_write_fp():
            encoder.encode(doc_list, file_obj)

    @classmethod
    def restore(cls, document_class: Type[BaseDocument], location: str = None, data_encode: str = None,
                data_format: str = None, data_store: str = None, **kwargs):
        """Restore data of a model


        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            data_encode (str): Backup Data Code
            data_format (str): Backup Data Format
            data_store (str): Backup Data Store location
            location(str): Data location to e used by data store
            **kwargs: parameter to be passed at engine level
        """
        assert cls.backup_coder,  "Backup encoder must be set to use backup function"
        assert cls.backup_storer,  "Backup store must be set to use backup function"
        decoder = cls.backup_coder(document_class, data_encode=data_encode, data_format=data_format)
        storer = cls.backup_storer(location=location, data_store=data_store)
        for file_obj in storer.get_read_fp():
            query = decoder.parse_content(file_obj)
            # Save the parsed data
            for doc in query:
                doc.save()

    @classmethod
    def create(cls, document_class: Type[BaseDocument], db_content: dict, doc_id: str = None) -> str:
        """Create a document

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            db_content (dict): content to be put to engine
            doc_id (str): Having predefined doc id, None means could be generated by engine

        Returns:
            Document ID
        """
        return str(uuid.uuid4()) if doc_id is None else doc_id

    @classmethod
    def get(cls, document_class: Type[BaseDocument], doc_id: str) -> dict:
        """Get Document

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            doc_id: Document ID

        Returns:
            Document content on python dict
        """

    @classmethod
    def set(cls, document_class: Type[BaseDocument], doc_id: str, db_content: dict) -> str:
        """Overwrite whole document

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            doc_id: Document ID
            db_content: content to be put to engine

        Returns:
            Document ID
        """

    @classmethod
    def update_doc_id(cls, document_class: Type[BaseDocument], db_content: dict, old_id: str, new_id: str):
        """Update document id to new value

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            db_content: content to be put to new engine
            old_id: old document id
            new_id: new document id

        Returns:
            new_document_id if the process is successful

        Comments:
            By default, we return old id(not implemented).
            When it is implemented in the Engine, will return new document id
        """
        return old_id

    @classmethod
    def update(cls, _document_class: Type[BaseDocument], _doc_id: str, **kwargs) -> dict:
        """Update a document

        Args:
            _document_class (`subclass` of `BaseDocument`): Document definition
            _doc_id (str): Document ID
            **kwargs: Named keyword for update

        Returns:
            Updated data

        Notes for delete string:
            * embedded update: a__b means b component of a. a.b means the key's name is a.b
            * operators: key is end with __op__. The following op are supported:
                * __append__: Append an item to array
                * __remove__: Remove an item
                * __delete__: Delete the field
        """
        return {}

    @classmethod
    def scan(cls, _document_class: Type[BaseDocument], _acl_queries: list = None, _limit: int = 1000, **kwargs):
        """Scan the document class and get the document id list

        Args:
            _document_class (`subclass` of `BaseDocument`): Document definition
            _acl_queries (list): Extra queries calculated from user's access control list
            _limit (int): Limited the scan results
            **kwargs: Named arguments are search string

        Notes for search string:
            * key, str pair: single value search
            * key, list pair: array_contains_any search
            * embedded search: a__b means b component of a. a.b means the key's name is a.b
            * operators: key is end with __op__. The following op are supported:
                * __eq__: Could ignore because it is a by default behavior
                * __lt__, __le__, __gt__, __ge__, __ne__: as is supposed by the name
                * __asc__, __desc__: the result will be ordered by the fields

        Attentions:
            * The complex query might raise compatible issues
        """
        return []

    @classmethod
    def fetch(cls, document_class: Type[BaseDocument], *args):
        """Get document one by one from a list of document id

        Returns:
            An iterator for id, document dictionary pair

        Comments:
            when doc id is empty, it is probably because that the user only has partial read authorizations
        """
        if not args:
            return []
        for doc_dict in cls.search(document_class, *args, _acl_queries=[{}]):
            yield doc_dict.get("_id", ""), doc_dict

    @classmethod
    def _acl_query_filter(cls, document_class: Type[BaseDocument], acl_queries: list, db_content: dict) -> bool:
        """Filter Access Control List. Should only be used if this job could not be done at engine level

        Args:
            document_class: Document Class
            acl_queries: Access Control Query
            db_content: Database contents retrieve from database

        Returns:
            True if content passed the control

        Attention:
            We suppose the all data in the acl queries has the same format as the output of database.
            If it is not the case, this function must be rewritten
        """
        return any(all(item in db_content.items() for item in query.items()) for query in acl_queries)

    @classmethod
    def search(cls, _document_class: Type[BaseDocument], *args, _acl_queries: list = None,
               _limit: int = 50, **kwargs):
        """Searching and yield document by document

        Args:
            _document_class (`subclass` of `BaseDocument`): Document definition
            *args: Unnamed arguments are document id
            _acl_queries: Extra queries calculated from user's Access Control List
            _limit: Search result is limited
            **kwargs: Named arguments are search string

        Notes for search string:
            * key, str pair: single value search
            * key, list pair: array_contains_any search
            * embedded search: a__b means b component of a. a.b means the key's name is a.b
            * operators: key is end with __op__. The following op are supported:
                * __eq__: Could ignore because it is a by default behavior
                * __lt__, __le__, __gt__, __ge__, __ne__: as is supposed by the name
                * __asc__, __desc__: the result will be ordered by the fields

        Attentions:
            * The complex query might raise compatible issues
        """
        return []

    @classmethod
    def delete(cls, document_class: Type[BaseDocument], doc_id: str):
        """Delete a document by using id

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            doc_id: Document ID
        """

    @classmethod
    def merge(cls, document_class: Type[BaseDocument], start: float = None, end: float = None,
              purge: bool = False, criteria: dict = None):
        """Merge data from log section into main table

        Args:
            document_class: (`subclass` of `BaseDocument`): Document definition
            start (timestamp): Starting time point
            end (timestamp): Ending time point
            purge: will remove the entries from log table after execution
            criteria: only merge the given criteria

        Comments:
            This method is designed to keep a high consistency data. All replicated data is kept on the log table.
            Only merge the data into main table when passed the consistency check
        """

    @classmethod
    def truncate(cls, document_class: Type[BaseDocument]):
        """Remove all data from the given collection

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
        """

    @classmethod
    def drop(cls, document_class: Type[BaseDocument]):
        """Drop the given collection

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
        """

    @classmethod
    def compile(cls, document_class: Type[BaseDocument], analytic_request: dict, acl_condition=None):
        """Compile the analysis request

        Args:
            document_class (`subclass` of `BaseDocument`): Document definition
            analytic_request: analytic request
            acl_condition: User Access List transformed to where conditions

        Returns:
            A analytic model ready to be executed represented by as dict {Engine: Model}
        """
        if cls.analyzer:
            return cls.analyzer.compile(document_class, cls, analytic_request, acl_condition)
        else:
            return {}

    @classmethod
    def analyze(cls, document_class: Type[BaseDocument], analytic_model: dict):
        """Run the analytic model

        Args:
            analytic_model: Analyze model
            document_class: (`subclass` of `BaseDocument`): Document definition

        """
        return []


class BaseLogger(Engine):
    """Log data changes"""
    log_class = None  #: Generating Class to hold the log

    @classmethod
    def get_start_sequence(cls, doc: BaseDocument) -> str:
        """Get start sequence for the given document

        Args:
            doc: Document Object

        Returns:
            Start sequence String
        """
        return datetime.utcnow().strftime('%Y%m%d%H%M%S%f')

    @classmethod
    def generate_log(cls,
                     doc: BaseDocument,
                     operation_type: str,
                     *,
                     insert_timestamp: float = None,
                     with_data: bool = False,
                     logger_name: str = None,
                     data_encode: str = None,
                     data_format: str = None,
                     data_store: str = None):
        """Generate log from document or a list of document

        Args:
            doc: document or document list
            operation_type: Type of operation
            insert_timestamp: Timestamp of data insertion
            with_data: Extract data with log
            logger_name: could specify logger name
            data_encode: Wanted data encode
            data_format: Wanted data format
            data_store: Where the data should be stored

        Returns:
            Logger Object Ready to save
        """


class MetaCache(MetaEngine):
    def __new__(mcs, *args, **kwargs):
        cls = super().__new__(mcs, *args, **kwargs)
        cls._cache_info = {}
        return cls


class BaseCache(Engine, metaclass=MetaCache):
    _cache_info = {}

    @classmethod
    def record_hit(cls, collection_name):
        """Record that the cache hits

        Args:
            collection_name: Collection Name
        """
        if collection_name not in cls._cache_info:
            cls._cache_info[collection_name] = {"hits": 0, "misses": 0, "current_size": 0}
        cls._cache_info[collection_name]["hits"] += 1

    @classmethod
    def record_miss(cls, collection_name):
        """Record that the cache misses

        Args:
            collection_name: Collection Name
        """
        if collection_name not in cls._cache_info:
            cls._cache_info[collection_name] = {"hits": 0, "misses": 0, "current_size": 0}
        cls._cache_info[collection_name]["misses"] += 1

    @classmethod
    def record_size(cls, collection_name):
        """Get a new cache size of given collection

        Args:
            collection_name: Collection Name
        """


class DummyEngine(Engine):
    """Dummy Engine is a special engine which does nothing"""
    engine_param = "dummy"


class MetaRamEngine(MetaEngine):
    def __new__(mcs, *args, **kwargs):
        cls = super().__new__(mcs, *args, **kwargs)
        cls._storage = {}
        return cls


class RamEngine(Engine, metaclass=MetaRamEngine):
    _storage = {}  #: Ram document storage
    _lock_dict = defaultdict(lambda: {"lock": threading.Lock(), "event": threading.Event()})
    _max_lock_duration = 2  # Auto release in 2 seconds

    engine_param = "ram"

    @classmethod
    def timed_release(cls, class_name, doc_id):
        event = cls._lock_dict[(class_name, doc_id)]["event"]
        event.wait(cls._max_lock_duration)
        if cls._lock_dict[(class_name, doc_id)]["lock"].locked():
            cls._lock_dict[(class_name, doc_id)]["lock"].release()

    @classmethod
    def lock(cls, document_class: Type[BaseDocument], doc_id: str, timeout: int = None):
        lock_acquired = cls._lock_dict[(document_class.__name__, doc_id)]["lock"].acquire(timeout=timeout)
        if lock_acquired:
            release_thread = threading.Thread(target=cls.timed_release, args=(document_class.__name__, doc_id))
            release_thread.start()
            return True, ""
        else:
            return False, f"Could not acquire lock for '{document_class.__name__}/{doc_id}' within timeout {timeout}s"

    @classmethod
    def unlock(cls, document_class: Type[BaseDocument], doc_id: str):
        lock = cls._lock_dict.get((document_class.__name__, doc_id), {"lock": None})
        if lock and lock["lock"].locked():
            lock["lock"].release()
        return True, ""

    @classmethod
    def _drop(cls, storage: dict, collection_name: str):
        storage.pop(collection_name, None)

    @classmethod
    def drop(cls, document_class: Type[BaseDocument]):
        collection_name = document_class.get_collection_name(cls)
        return cls._drop(cls._storage, collection_name)

    @classmethod
    def _create(cls, storage: dict, collection_name: str, db_content: dict, doc_id: str = None) -> str:
        if collection_name not in storage:
            storage[collection_name] = OrderedDict()
        doc_id = str(uuid.uuid4()) if not doc_id else doc_id
        storage[collection_name][doc_id] = db_content
        return doc_id

    @classmethod
    def create(cls, document_class: Type[BaseDocument], db_content: dict, doc_id: str = None) -> str:
        collection_name = document_class.get_collection_name(cls)
        return cls._create(cls._storage, collection_name, db_content, doc_id)

    @classmethod
    def _get(cls, storage: dict, collection_name: str, doc_id: str) -> dict:
        doc_dict = storage.get(collection_name, {}).get(doc_id, None)
        if doc_dict is not None:
            doc_dict["_id"] = doc_id
            return doc_dict

    @classmethod
    def get(cls, document_class: Type[BaseDocument], doc_id: str) -> dict:
        collection_name = document_class.get_collection_name(cls)
        return cls._get(cls._storage, collection_name, doc_id)

    @classmethod
    def _set(cls, storage: dict, collection_name: str, doc_id: str, db_content: dict) -> str:
        if collection_name not in storage:
            storage[collection_name] = OrderedDict()
        storage[collection_name][doc_id] = db_content
        return doc_id

    @classmethod
    def set(cls, document_class: Type[BaseDocument], doc_id: str, db_content: dict) -> str:
        collection_name = document_class.get_collection_name(cls)
        return cls._set(cls._storage, collection_name, doc_id, db_content)

    @classmethod
    def _update(cls, storage: dict, collection_name: str, doc_id: str, **kwargs) -> dict:
        todo_object = storage[collection_name][doc_id]
        for key, value in kwargs.items():
            field_name, operator = cls.parse_update_option(key)
            if operator is None:
                todo_object[field_name] = value
            elif operator == "append":
                todo_object[field_name] = [] if field_name not in todo_object else todo_object[field_name]
                todo_object[field_name].extend(value if isinstance(value, list) else [value])
            elif operator == "remove":
                origin_list = todo_object.get(field_name, [])
                remove_list = value if isinstance(value, list) else [value]
                todo_object[field_name] = [item for item in origin_list if item not in remove_list]
            elif operator == "delete":
                todo_object.pop(field_name, None)
        return todo_object

    @classmethod
    def update(cls, _document_class: Type[BaseDocument], _doc_id: str, **kwargs) -> dict:
        collection_name = _document_class.get_collection_name(cls)
        return cls._update(cls._storage, collection_name, _doc_id, **kwargs)

    @classmethod
    def _fetch(cls, storage: dict, collection_name: str, *args):
        if collection_name not in storage:
            return []
        database = storage[collection_name].copy()
        for doc_id in args:
            if doc_id in database:
                doc_dict = database[doc_id]
                doc_dict["_id"] = doc_id
                yield doc_id, doc_dict

    @classmethod
    def fetch(cls, document_class: Type[BaseDocument], *args):
        collection_name = document_class.get_collection_name(cls)
        for doc_id, doc_dict in cls._fetch(cls._storage, collection_name, *args):
            yield doc_id, doc_dict

    @classmethod
    @result_limiter
    def _search(cls, storage: dict, collection_name: str, *args, _acl_queries: list = None,
                _limit: int = 50, **kwargs):
        if collection_name not in storage:
            return []
        database = storage[collection_name].copy()
        if args:
            for doc_id, doc_dict in cls._fetch(storage, collection_name, *args):
                if cls._acl_query_filter(BaseDocument, _acl_queries, doc_dict):
                    yield doc_dict  # Pass dummy Document instance because no conversion is needed for RamEngine
        else:
            for doc_id, doc_dict in database.items():
                match = True
                for key, value in kwargs.items():
                    field, operator, order = cls.parse_search_option(key)
                    field_value = doc_dict.get(field, None)
                    if isinstance(field_value, list) or isinstance(value, list):
                        field_value = field_value if isinstance(field_value, list) else [field_value]
                        value = value if isinstance(value, list) else [value]
                        if not set(field_value) & set(value):  # Array contains Any
                            match = False
                            break
                    elif operator == "==" and field_value != value:
                        match = False
                        break
                if match:
                    if cls._acl_query_filter(BaseDocument, _acl_queries, doc_dict):
                        doc_dict["_id"] = doc_id
                        yield doc_dict  # Pass dummy Document instance because no conversion is needed for RamEngine

    @classmethod
    def search(cls, _document_class: Type[BaseDocument], *args, _acl_queries: list = None,
               _limit: int = 50, **kwargs):
        collection_name = _document_class.get_collection_name(cls)
        for doc_dict in cls._search(
                cls._storage, collection_name, *args, _acl_queries=_acl_queries, _limit=_limit, **kwargs
        ):
            yield doc_dict

    @classmethod
    def scan(cls, _document_class: Type[BaseDocument], _acl_queries: list = None, _limit: int = 1000, **kwargs):
        collection_name = _document_class.get_collection_name(cls)
        scan_result = [doc_dict["_id"] for doc_dict in cls._search(
            cls._storage, collection_name, _acl_queries=_acl_queries, _limit=_limit, **kwargs
        )]
        return scan_result

    @classmethod
    def _delete(cls, storage: dict, collection_name: str, doc_id: str):
        storage[collection_name].pop(doc_id, None)

    @classmethod
    def delete(cls, document_class: Type[BaseDocument], doc_id: str):
        collection_name = document_class.get_collection_name(cls)
        return cls._delete(cls._storage, collection_name, doc_id)

    @classmethod
    def batch(cls, operations: list, originals: dict):
        rollback_ops = []
        try:
            for operation in operations:
                if operation["op"] == "I":
                    rollback_op = {"op": "D", "cls": operation["cls"], "doc_id": operation["doc_id"]}
                    cls.create(operation["cls"], operation["content"], operation["doc_id"])
                    rollback_ops.append(rollback_op)
                elif operation["op"] == "U":
                    rollback_op = {"op": "S", "cls": operation["cls"], "doc_id": operation["doc_id"],
                                   "content": originals[(operation["cls"], operation["doc_id"])]["content"]}
                    cls.update(operation["cls"], operation["doc_id"], **operation["content"])
                    rollback_ops.append(rollback_op)
                elif operation["op"] == "S":
                    rollback_op = {"op": "S", "cls": operation["cls"], "doc_id": operation["doc_id"],
                                   "content": originals[(operation["cls"], operation["doc_id"])]["content"]}
                    cls.set(operation["cls"], operation["doc_id"], operation["content"])
                    rollback_ops.append(rollback_op)
                elif operation["op"] == "D":
                    rollback_op = {"op": "I", "cls": operation["cls"], "doc_id": operation["doc_id"],
                                   "content": originals[(operation["cls"], operation["doc_id"])]["content"]}
                    cls.delete(operation["cls"], operation["doc_id"])
                    rollback_ops.append(rollback_op)
            return True, ""
        except Exception as e:
            for operation in rollback_ops:
                if operation["op"] == "I":
                    cls.create(operation["cls"], operation["content"], operation["doc_id"])
                elif operation["op"] == "U":
                    cls.update(operation["cls"], operation["doc_id"], **operation["content"])
                elif operation["op"] == "S":
                    cls.set(operation["cls"], operation["doc_id"], operation["content"])
                elif operation["op"] == "D":
                    cls.delete(operation["cls"], operation["doc_id"])
            return False, str(e)


class BatchEngine(RamEngine):
    """Context engine is used to store batch related information"""

    @classmethod
    def drop_batch(cls, batch_id: str = ""):
        """Drop the database which holds batch id

        Args:
            batch_id: Batch Identity
        """
        cls._storage.pop(batch_id, None)

    @classmethod
    def drop(cls, document_class: Type[BaseDocument], batch_id: str = ""):
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._drop(cls._storage[batch_id], collection_name)

    @classmethod
    def create(cls, document_class: Type[BaseDocument], db_content: dict, doc_id: str = None, batch_id: str = ""):
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._create(cls._storage[batch_id], collection_name, db_content, doc_id)

    @classmethod
    def get(cls, document_class: Type[BaseDocument], doc_id: str, batch_id: str = "") -> dict:
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._get(cls._storage[batch_id], collection_name, doc_id)

    @classmethod
    def set(cls, document_class: Type[BaseDocument], doc_id: str, db_content: dict, batch_id: str = "") -> str:
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._set(cls._storage[batch_id], collection_name, doc_id, db_content)

    @classmethod
    def update(cls, _document_class: Type[BaseDocument], _doc_id: str, batch_id: str = "", **kwargs) -> dict:
        collection_name = _document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._update(cls._storage[batch_id], collection_name, _doc_id, **kwargs)

    @classmethod
    def fetch(cls, document_class: Type[BaseDocument], batch_id: str = "", *args):
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        for doc_id, doc_dict in cls._fetch(cls._storage[batch_id], collection_name, *args):
            yield doc_id, doc_dict

    @classmethod
    def scan(cls, _document_class: Type[BaseDocument], _batch_id: str = "", _acl_queries: list = None,
             _limit: int = 1000, **kwargs):
        collection_name = _document_class.get_collection_name(cls)
        _acl_queries = [{}] if not _acl_queries else _acl_queries
        if _batch_id not in cls._storage:
            cls._storage[_batch_id] = {}
        scan_result = [doc_dict["_id"] for doc_dict in cls._search(
            cls._storage[_batch_id], collection_name, _acl_queries=_acl_queries, _limit=_limit, **kwargs
        )]
        return scan_result

    @classmethod
    def search(cls, _document_class: Type[BaseDocument], *args, _batch_id: str = "",
               _acl_queries: list = None, _limit: int = 50, **kwargs):
        collection_name = _document_class.get_collection_name(cls)
        _acl_queries = [{}] if not _acl_queries else _acl_queries
        if _batch_id not in cls._storage:
            cls._storage[_batch_id] = {}
        for doc_dict in cls._search(
                cls._storage[_batch_id], collection_name, *args, _acl_queries=_acl_queries, _limit=_limit, **kwargs
        ):
            yield doc_dict

    @classmethod
    def delete(cls, document_class: Type[BaseDocument], doc_id: str, batch_id: str = ""):
        collection_name = document_class.get_collection_name(cls)
        if batch_id not in cls._storage:
            cls._storage[batch_id] = {}
        return cls._delete(cls._storage[batch_id], collection_name, doc_id)


class MirrorClient:
    def __init__(self, primary, secondaries: list = None):
        self.primary = primary
        self.secondaries = secondaries if secondaries else []


class MirrorEngine(Engine):
    """Mirror Engine is an engine which has primary and secondary components.
    """
    engine_connector = MirrorClient
    engine_param = "mirror"

    @classmethod
    def create(cls, document_class: Type[BaseDocument], db_content: dict, doc_id: str = None) -> str:
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.create, args=(document_class, db_content, doc_id))
            thread.start()
            threads.append(thread)
        result = db_con.primary.create(document_class, db_content=db_content, doc_id=doc_id)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def get(cls, document_class: Type[BaseDocument], doc_id: str) -> dict:
        db_con = cls.get_connection(document_class)
        return db_con.primary.get(document_class, doc_id=doc_id)

    @classmethod
    def set(cls, document_class: Type[BaseDocument], doc_id: str, db_content: dict) -> str:
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.set, args=(document_class, doc_id, db_content))
            thread.start()
            threads.append(thread)
        result = db_con.primary.set(document_class, doc_id=doc_id, db_content=db_content)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def update(cls, _document_class: Type[BaseDocument], _doc_id: str, **kwargs) -> dict:
        db_con = cls.get_connection(_document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.update, args=(_document_class, _doc_id), kwargs=kwargs)
            thread.start()
            threads.append(thread)
        result = db_con.primary.update(_document_class, _doc_id=_doc_id, **kwargs)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def delete(cls, document_class: Type[BaseDocument], doc_id: str):
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.delete, args=(document_class, doc_id))
            thread.start()
            threads.append(thread)
        result = db_con.primary.delete(document_class, doc_id=doc_id)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def search(cls, _document_class: Type[BaseDocument], *args, _acl_queries: list = None,
               _limit: int = 50, **kwargs):
        db_con = cls.get_connection(_document_class)
        return db_con.primary.search(_document_class, *args, _acl_queries=_acl_queries, _limit=_limit, **kwargs)

    @classmethod
    def lock(cls, document_class: Type[BaseDocument], doc_id: str, timeout: int = None):
        db_con = cls.get_connection(document_class)
        return db_con.primary.lock(document_class, doc_id=doc_id, timeout=timeout)

    @classmethod
    def unlock(cls, document_class: Type[BaseDocument], doc_id: str):
        db_con = cls.get_connection(document_class)
        return db_con.primary.unlock(document_class, doc_id=doc_id)

    @classmethod
    def batch(cls, operations: list, originals: dict):
        db_con = cls.get_connection()
        result = db_con.primary.batch(operations, originals)
        for secondary in db_con.secondaries:
            secondary.batch(operations, originals)
        return result

    @classmethod
    def create_collection(cls, document_class: Type[BaseDocument]):
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.create_collection, args=(document_class,))
            thread.start()
            threads.append(thread)
        result = db_con.primary.create_collection(document_class)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def backup(cls, document_class: Type[BaseDocument], location: str = None, data_encode: str = None,
               data_format: str = None, data_store: str = None, **kwargs):
        db_con = cls.get_connection(document_class)
        return db_con.primary.backup(document_class, location=location, data_encode=data_encode,
                                     data_format=data_format, data_store=data_store, **kwargs)

    @classmethod
    def restore(cls, document_class: Type[BaseDocument], location: str = None, data_encode: str = None,
                data_format: str = None, data_store: str = None, **kwargs):
        db_con = cls.get_connection(document_class)
        return db_con.primary.restore(document_class, location=location, data_encode=data_encode,
                                      data_format=data_format, data_store=data_store, **kwargs)

    @classmethod
    def scan(cls, _document_class: Type[BaseDocument], _acl_queries: list = None, _limit: int = 1000, **kwargs):
        db_con = cls.get_connection(_document_class)
        return db_con.primary.scan(_document_class, _acl_queries=_acl_queries, _limit=_limit, **kwargs)

    @classmethod
    def fetch(cls, document_class: Type[BaseDocument], *args):
        db_con = cls.get_connection(document_class)
        return db_con.primary.fetch(document_class, *args)

    @classmethod
    def merge(cls, document_class: Type[BaseDocument], start: float = None, end: float = None,
              purge: bool = False, criteria: dict = None):
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.merge, args=(document_class, start, end, purge, criteria))
            thread.start()
            threads.append(thread)
        result = db_con.primary.merge(document_class, start=start, end=end, purge=purge, criteria=criteria)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def truncate(cls, document_class: Type[BaseDocument]):
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.truncate, args=(document_class,))
            thread.start()
            threads.append(thread)
        result = db_con.primary.truncate(document_class)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def drop(cls, document_class: Type[BaseDocument]):
        db_con = cls.get_connection(document_class)
        threads = []
        for secondary in db_con.secondaries:
            thread = threading.Thread(target=secondary.drop, args=(document_class,))
            thread.start()
            threads.append(thread)
        result = db_con.primary.drop(document_class)
        for thread in threads:
            thread.join()
        return result

    @classmethod
    def compile(cls, document_class: Type[BaseDocument], analytic_request: dict, acl_condition=None):
        db_con = cls.get_connection(document_class)
        return db_con.primary.compile(document_class, analytic_request=analytic_request, acl_condition=acl_condition)

    @classmethod
    def analyze(cls, document_class: Type[BaseDocument], analytic_model: dict):
        db_con = cls.get_connection(document_class)
        return db_con.primary.analyze(document_class, analytic_model=analytic_model)
